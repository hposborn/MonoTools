{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DYLD_LIBRARY_PATH=/Users/hosborn/python/MultiNest/lib\n"
     ]
    }
   ],
   "source": [
    "%set_env DYLD_LIBRARY_PATH=/Users/hosborn/python/MultiNest/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "dlsym(RTLD_DEFAULT, run): symbol not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff73e3bf8bd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# run MultiNest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mpymultinest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloglike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/transits/lib/python3.7/site-packages/pymultinest-2.9-py3.7.egg/pymultinest/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(LogLikelihood, Prior, n_dims, n_params, n_clustering_params, wrapped_params, importance_nested_sampling, multimodal, const_efficiency_mode, n_live_points, evidence_tolerance, sampling_efficiency, n_iter_before_update, null_log_evidence, max_modes, mode_tolerance, outputfiles_basename, seed, verbose, resume, context, write_output, log_zero, max_iter, init_MPI, dump_callback)\u001b[0m\n\u001b[1;32m    252\u001b[0m \t\tlog_zero, max_iter, loglike, dumper, context]\n\u001b[1;32m    253\u001b[0m         \u001b[0margs_converted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_converted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'mpi'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlibname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0;31m# wait for all processes to return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name_or_ordinal)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_ordinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FuncPtr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_ordinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_ordinal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_or_ordinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: dlsym(RTLD_DEFAULT, run): symbol not found"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy\n",
    "from numpy import log, exp, pi\n",
    "import scipy.stats, scipy\n",
    "import pymultinest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we define the problem: we need a prior function which maps from [0:1] to the parameter space\n",
    "\n",
    "# we only have one parameter, the position of the gaussian (ndim == 1)\n",
    "# map it from the unity interval 0:1 to our problem space 0:2 under a uniform prior\n",
    "def prior(cube, ndim, nparams):\n",
    "    cube[0] = cube[0] * 2\n",
    "\n",
    "# our likelihood function consists of 6 gaussians modes (solutions) at the positions\n",
    "positions = numpy.array([0.1, 0.2, 0.5, 0.55, 0.9, 1.1])\n",
    "width = 0.01\n",
    "\n",
    "def loglike(cube, ndim, nparams):\n",
    "    # get the current parameter (is between 0:2 now)\n",
    "    pos = cube[0]\n",
    "    likelihood = exp(-0.5 * ((pos - positions) / width)**2) / (2*pi*width**2)**0.5\n",
    "    return log(likelihood.mean())\n",
    "\n",
    "# number of dimensions our problem has\n",
    "parameters = [\"position\"]\n",
    "n_params = len(parameters)\n",
    "\n",
    "# run MultiNest\n",
    "pymultinest.run(loglike, prior, n_params, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.linspace(0, 1, 400)\n",
    "ydata = None # loaded below\n",
    "\n",
    "noise = 0.1\n",
    "\n",
    "np.loadtxt(\"/Users/hosborn/python/PyMultiNest/pymultinest/\")\n",
    "\n",
    "# model for 2 gaussians, same width, fixed offset\n",
    "def model(pos1, width, height1, height2):\n",
    "    pos2 = pos1 + 0.05\n",
    "    return  height1 * scipy.stats.norm.pdf(x, pos1, width) + \\\n",
    "        height2 * scipy.stats.norm.pdf(x, pos2, width)\n",
    "\n",
    "# a more elaborate prior\n",
    "# parameters are pos1, width, height1, [height2]\n",
    "def prior(cube, ndim, nparams):\n",
    "    #cube[0] = cube[0]            # uniform prior between 0:1\n",
    "    cube[1] = 10**(cube[1]*8 - 4) # log-uniform prior between 10^-4 and 10^4\n",
    "    cube[2] = 10**(cube[2]*4 - 4) # log-uniform prior between 10^-4 and 1\n",
    "    if ndim < 4:\n",
    "        return\n",
    "    cube[3] = 10**(cube[3]*4 - 4) # log-uniform prior between 10^-4 and 1\n",
    "\n",
    "\n",
    "def loglike(cube, ndim, nparams):\n",
    "    pos1, width, height1 = cube[0], cube[1], cube[2]\n",
    "    height2 = cube[3] if ndim > 3 else 0\n",
    "    ymodel = model(pos1, width, height1, height2)\n",
    "    loglikelihood = (-0.5 * ((ymodel - ydata) / noise)**2).sum()\n",
    "    return loglikelihood\n",
    "\n",
    "parameters = [\"pos1\", \"width\", \"height1\"]\n",
    "n_params = len(parameters)\n",
    "\n",
    "pymultinest.run(loglike, prior, n_params, outputfiles_basename='test_1_', resume = False, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm,beta,truncnorm\n",
    "\n",
    "def transform_uniform(x,a,b):\n",
    "    return a + (b-a)*x\n",
    "\n",
    "def transform_loguniform(x,a,b):\n",
    "    la=np.log(a)\n",
    "    lb=np.log(b)\n",
    "    return np.exp(la + x*(lb-la))\n",
    "\n",
    "def transform_normal(x,mu,sigma):\n",
    "    return norm.ppf(x,loc=mu,scale=sigma)\n",
    "\n",
    "def transform_beta(x,a,b):\n",
    "    return beta.ppf(x,a,b)\n",
    "\n",
    "def transform_truncated_normal(x,mu,sigma,a=0.,b=1.):\n",
    "    ar, br = (a - mu) / sigma, (b - mu) / sigma\n",
    "    return truncnorm.ppf(x,ar,br,loc=mu,scale=sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinest():\n",
    "    parameters = [\"x\", \"y\"]\n",
    "    n_params = len(parameters)\n",
    "    args=(x_data, y_data, err_data)\n",
    "    \n",
    "    per_index=-8/3\n",
    "    \n",
    "    def myprior(cube, ndim, nparams):\n",
    "        # do prior transformation\n",
    "        start=None\n",
    "\n",
    "        ######################################\n",
    "        #   Intialising Stellar Params:\n",
    "        ######################################\n",
    "        #Using log rho because otherwise the distribution is not normal:\n",
    "        cube_indeces={}\n",
    "        cube_indeces['logrho_S']=nparams;nparams+=1\n",
    "        cube[0] = transform_normal(cube[0],mu=np.log(self.rhostar[0]), \n",
    "                                    sd=np.average(abs(self.rhostar[1:]/self.rhostar[0])))\n",
    "        cube_indeces['Rs']=nparams;nparams+=1\n",
    "        cube[cube_indeces['Rs']] = transform_normal(cube[cube_indeces['Rs']],mu=self.Rstar[0],\n",
    "                                                    sd=np.average(abs(self.Rstar[1:])))\n",
    "        # The baseline flux\n",
    "        cube_indeces['mean']=nparams;nparams+=1\n",
    "        cube_indeces['mean']=transform_normal(cube[cube_indeces['mean']],mu=np.median(self.lc['flux'][self.lc['mask']]),\n",
    "                                              sd=np.std(self.lc['flux'][self.lc['mask']]))\n",
    "\n",
    "        # The 2nd light (not third light as companion light is not modelled) \n",
    "        # This quantity is in delta-mag\n",
    "        if useL2:\n",
    "            cube_indeces['deltamag_contam']=nparams;nparams+=1\n",
    "            cube[cube_indeces['deltamag_contam']] = transform_uniform(cube[cube_indeces['deltamag_contam']], lower=-20.0, upper=20.0)\n",
    "            mult = (1+np.power(2.511,-1*cube[cube_indeces['deltamag_contam']])) #Factor to multiply normalised lightcurve by\n",
    "        else:\n",
    "            mult=1.0\n",
    "\n",
    "        print(\"Forming Pymc3 model with: monos:\",self.monos,\"multis:\",self.multis,\"duos:\",self.duos)\n",
    "\n",
    "        ######################################\n",
    "        #     Initialising Periods & tcens\n",
    "        ######################################\n",
    "        for pl in self.multis+self.monos+self.duos:\n",
    "            tcen=self.planets[pl]['tcen']\n",
    "            tdur=self.planets[pl]['tdur']\n",
    "            cube_indeces['t0_'+pl]=nparams;nparams+=1\n",
    "            cube[cube_indeces['t0_'+pl]]=transform_truncated_normal(cube[cube_indeces['t0_'+pl]],\n",
    "                                                                    mu=tcen,sigma=tdur*0.1,\n",
    "                                                                    a=tcen-tdur*0.5,b=tcen+tdur*0.5)\n",
    "            if pl in self.monos:\n",
    "                cube_indeces['per_'+pl]=nparams;nparams+=1\n",
    "                #Need to loop through possible period gaps, according to their prior probability *density* (hence the final term diiding by gap width)\n",
    "                rel_gap_prob = (self.planets[pl]['per_gaps'][:,0]**(-5/3)-self.planets[pl]['per_gaps'][:,1]**(-5/3))/self.planets[pl]['per_gaps'][:,2]\n",
    "                rel_gap_prob /= np.sum(rel_gap_prob)\n",
    "                rel_gap_prob = np.hstack((0.0,np.cumsum(rel_gap_prob)))\n",
    "                incube=cube[cube_indeces['per_'+pl]]\n",
    "                outcube=incube[:]\n",
    "                #looping through each gpa, cutting the \"cube\" up according to their prior probability and making a p~P^-8/3 distribution for each gap:\n",
    "                for i in range(len(rel_gap_prob)-1):\n",
    "                    ind_min=np.power(self.planets[pl]['per_gaps'][i,1]/self.planets[pl]['per_gaps'][i,0],per_index)\n",
    "                    outcube[(incube>relgap_prob[i])&(incube<=rel_gap_prob[i+1])]=np.power(((1-ind_min)*(incube[(incube>rel_gap_prob[i])&(incube<=rel_gap_prob[i+1])]-rel_gap_prob[i])/(rel_gap_prob[i+1]-rel_gap_prob[i])+ind_min),1/per_index)*self.planets[pl]['per_gaps'][i,0]\n",
    "                cube[cube_indeces['per_'+pl]]=outcube\n",
    "            # The period distributions of monotransits are tricky as we often have gaps to contend with\n",
    "            # We cannot sample the full period distribution while some regions have p=0.\n",
    "            # Therefore, we need to find each possible period region and marginalise over each\n",
    "\n",
    "            if pl in self.duos:\n",
    "                #In the case of a duotransit, we have a discrete series of possible periods between two know transits.\n",
    "                #If we want to model this duo transit exactly like the first (as a bounded normal)\n",
    "                # we can work out the necessary periods to cause these dips\n",
    "                cube_indeces['t0_2_'+pl]=nparams;nparams+=1\n",
    "                tcen2=self.planets[pls]['tcen_2']\n",
    "                cube[cube_indeces['t0_2_'+pl]]=transform_truncated_normal(cube[cube_indeces['t0_2_'+pl]],\n",
    "                                                                        mu=tcen2,sigma=tdur*0.1,\n",
    "                                                                        a=tcen2-tdur*0.5,b=tcen2+tdur*0.5)\n",
    "                cube_indeces['duo_per_int_'+pl]=nparams;nparams+=1\n",
    "                rel_per_prob = duo['period_aliases']**(-8/3)\n",
    "                rel_per_prob /= np.sum(rel_per_prob)\n",
    "                rel_per_prob = np.hstack((0.0,np.cumsum(rel_per_prob)))\n",
    "                incube=cube[cube_indeces['duo_per_int_'+pl]]\n",
    "                outcube=incube[:]\n",
    "                #looping through each gp,p cutting the \"cube\" up according to their prior probability and making a p~P^-8/3 distribution for each gap:\n",
    "                for i in range(len(rel_per_prob)-1):\n",
    "                    ind_min=np.power(self.planets[pl]['per_gaps'][i,1]/self.planets[pl]['per_gaps'][i,0],per_index)\n",
    "                    outcube[(incube>rel_per_prob[i])&(incube<=rel_per_prob[i+1])]=duo['period_int_aliases'][i]\n",
    "            if pl in self.multis:\n",
    "                cube_indeces['per_'+pl]=nparams;nparams+=1\n",
    "                p=self.planets[pls]['period']\n",
    "                perr=self.planets[pls]['period_err']\n",
    "                cube[cube_indeces['per_'+pl]]=transform_normal(cube[cube_indeces['per_'+pl]],\n",
    "                                                                        mu=p,\n",
    "                                                                        sigma=np.clip(perr*0.25,0.005,0.02*p))\n",
    "                #In the case of multitransiting plaets, we know the periods already, so we set a tight normal distribution\n",
    "            \n",
    "            ######################################\n",
    "            #     Initialising R_p & b\n",
    "            ######################################\n",
    "            # The Espinoza (2018) parameterization for the joint radius ratio and\n",
    "            # impact parameter distribution\n",
    "            rpl=self.planets[pl]['r_pl']/(109.1*self.Rstar[0])\n",
    "            maxr=1.0 if useL2 else 0.2\n",
    "            cube_indeces['r_pl_'+pl]=nparams;nparams+=1\n",
    "            cube[cube_indeces['r_pl_'+pl]] = transform_uniform(cube[cube_indeces['r_pl_'+pl]],0.0,maxr)\n",
    "            \n",
    "            cube_indeces['b_'+pl]=nparams;nparams+=1\n",
    "            cube[cube_indeces['b_'+pl]] = transform_uniform(cube[cube_indeces['b_'+pl]],0.0,1.0)\n",
    "            #We can do the adjustment for b later when sampling in the model\n",
    "\n",
    "        ######################################\n",
    "        #     Initialising Limb Darkening\n",
    "        ######################################\n",
    "        # Here we either constrain the LD params given the stellar info, OR we let exoplanet fit them\n",
    "        if len(np.unique([c[0] for c in self.cads]))==1:\n",
    "            if constrain_LD:\n",
    "                n_samples=1200\n",
    "                # Bounded normal distributions (bounded between 0.0 and 1.0) to constrict shape given star.\n",
    "\n",
    "                #Single mission\n",
    "                if 't' in np.unique([c[0].lower() for c in self.cads]):\n",
    "                    ld_dists=self.getLDs(n_samples=3000,mission='tess')\n",
    "                    cube_indeces['u_star_tess_0']=nparams;nparams+=1\n",
    "                    cube[cube_indeces['u_star_tess_0']] = transform_truncated_normal(cube[cube_indeces['u_star_tess_0']],\n",
    "                                                                                    mu=np.clip(np.nanmedian(ld_dists[:,0],axis=0),0,1),\n",
    "                                                                                    sd=np.clip(ld_mult*np.nanstd(ld_dists[:,0],axis=0),0.05,1.0),\n",
    "                                                                                    a=0.0,b=1.0)\n",
    "                    cube_indeces['u_star_tess_1']=nparams;nparams+=1\n",
    "                    cube[cube_indeces['u_star_tess_1']] = transform_truncated_normal(cube[cube_indeces['u_star_tess_1']],\n",
    "                                                                                    mu=np.clip(np.nanmedian(ld_dists[:,0],axis=0),0,1),\n",
    "                                                                                    sd=np.clip(ld_mult*np.nanstd(ld_dists[:,0],axis=0),0.05,1.0),\n",
    "                                                                                    a=0.0,b=1.0)\n",
    "                if 'k' in np.unique([c[0].lower() for c in self.cads]):\n",
    "                    ld_dists=self.getLDs(n_samples=3000,mission='kepler')\n",
    "                    cube_indeces['u_star_kep_0']=nparams;nparams+=1\n",
    "                    cube[cube_indeces['u_star_kep_0']] = transform_truncated_normal(cube[cube_indeces['u_star_kep_0']],\n",
    "                                                                                    mu=np.clip(np.nanmedian(ld_dists[:,0],axis=0),0,1),\n",
    "                                                                                    sd=np.clip(ld_mult*np.nanstd(ld_dists[:,0],axis=0),0.05,1.0),\n",
    "                                                                                    a=0.0,b=1.0)\n",
    "                    cube_indeces['u_star_kep_1']=nparams;nparams+=1\n",
    "                    cube[cube_indeces['u_star_kep_1']] = transform_truncated_normal(cube[cube_indeces['u_star_kep_1']],\n",
    "                                                                                    mu=np.clip(np.nanmedian(ld_dists[:,0],axis=0),0,1),\n",
    "                                                                                    sd=np.clip(ld_mult*np.nanstd(ld_dists[:,0],axis=0),0.05,1.0),\n",
    "                                                                                    a=0.0,b=1.0)\n",
    "            else:\n",
    "                if 't' in np.unique([c[0].lower() for c in self.cads]):\n",
    "                    cube_indeces['u_star_tess_0']=nparams;nparams+=1\n",
    "                    cube[cube_indeces['u_star_tess_0']] = transform_uniform(cube[cube_indeces['u_star_tess_0']],0.0,1.0)\n",
    "                    cube_indeces['u_star_tess_1']=nparams;nparams+=1\n",
    "                    cube[cube_indeces['u_star_tess_1']] = transform_uniform(cube[cube_indeces['u_star_tess_1']],0.0,1.0)\n",
    "                if 'k' in np.unique([c[0].lower() for c in self.cads]):\n",
    "                    cube_indeces['u_star_kep_0']=nparams;nparams+=1\n",
    "                    cube[cube_indeces['u_star_kep_0']] = transform_uniform(cube[cube_indeces['u_star_kep_0']],0.0,1.0)\n",
    "                    cube_indeces['u_star_kep_1']=nparams;nparams+=1\n",
    "                    cube[cube_indeces['u_star_kep_1']] = transform_uniform(cube[cube_indeces['u_star_kep_1']],0.0,1.0)\n",
    "\n",
    "        else:\n",
    "            if constrain_LD:\n",
    "                n_samples=1200\n",
    "                #Multiple missions - need multiple limb darkening params:\n",
    "                ld_dist_tess=self.getLDs(n_samples=3000,mission='tess')\n",
    "\n",
    "                u_star_tess = pm.Bound(pm.Normal, \n",
    "                                       lower=0.0, upper=1.0)(\"u_star_tess\", \n",
    "                                                             mu=np.clip(np.nanmedian(ld_dist_tess,axis=0),0,1),\n",
    "                                                             sd=np.clip(ld_mult*np.nanstd(ld_dist_tess,axis=0),0.05,1.0), \n",
    "                                                             shape=2,\n",
    "                                                             testval=np.clip(np.nanmedian(ld_dist_tess,axis=0),0,1))\n",
    "                ld_dist_kep=self.getLDs(n_samples=3000,mission='tess')\n",
    "\n",
    "                u_star_kep = pm.Bound(pm.Normal, \n",
    "                                       lower=0.0, upper=1.0)(\"u_star_kep\", \n",
    "                                                             mu=np.clip(np.nanmedian(ld_dist_kep,axis=0),0,1),\n",
    "                                                             sd=np.clip(ld_mult*np.nanstd(ld_dist_kep,axis=0),0.05,1.0), \n",
    "                                                             shape=2,\n",
    "                                                             testval=np.clip(np.nanmedian(ld_dist_kep,axis=0),0,1))\n",
    "            else:\n",
    "                # The Kipping (2013) parameterization for quadratic limb darkening paramters\n",
    "                u_star_tess = xo.distributions.QuadLimbDark(\"u_star_tess\", testval=np.array([0.3, 0.2]))\n",
    "                u_star_kep = xo.distributions.QuadLimbDark(\"u_star_kep\", testval=np.array([0.3, 0.2]))\n",
    "\n",
    "        ######################################\n",
    "        #     Initialising GP kernel\n",
    "        ######################################\n",
    "        log_flux_std=np.array([np.log(np.std(self.lc['flux'][self.lc['oot_mask']&(self.lc['cadence']==c)])) for c in self.cads]).ravel()\n",
    "        logs2 = pm.Normal(\"logs2\", mu = 2*log_flux_std, sd = np.tile(2.0,len(log_flux_std)), shape=len(log_flux_std))\n",
    "\n",
    "        if use_GP:\n",
    "            # Transit jitter & GP parameters\n",
    "            #logs2 = pm.Normal(\"logs2\", mu=np.log(np.var(y[m])), sd=10)\n",
    "            lcrange=self.lc['time'][self.lc['oot_mask']][-1]-self.lc['time'][self.lc['oot_mask']][0]\n",
    "            min_cad = np.min([np.nanmedian(np.diff(self.lc['time'][self.lc['oot_mask']&(self.lc['cadence']==c)])) for c in self.cads])\n",
    "            #freqs bounded from 2pi/minimum_cadence to to 2pi/(4x lc length)\n",
    "            logw0 = pm.Uniform(\"logw0\",lower=np.log((2*np.pi)/(4*lcrange)), \n",
    "                               upper=np.log((2*np.pi)/min_cad),testval=np.log((2*np.pi)/(lcrange)))\n",
    "\n",
    "            # S_0 directly because this removes some of the degeneracies between\n",
    "            # S_0 and omega_0 prior=(-0.25*lclen)*exp(logS0)\n",
    "            maxpower=np.log(np.nanmedian(abs(np.diff(self.lc['flux'][self.lc['oot_mask']]))))+1\n",
    "            logpower = pm.Uniform(\"logpower\",lower=-20,upper=maxpower,testval=maxpower-6)\n",
    "            print(\"input to GP power:\",maxpower-1)\n",
    "            logS0 = pm.Deterministic(\"logS0\", logpower - 4 * logw0)\n",
    "\n",
    "            # GP model for the light curve\n",
    "            kernel = xo.gp.terms.SHOTerm(log_S0=logS0, log_w0=logw0, Q=1/np.sqrt(2))\n",
    "\n",
    "        if not assume_circ:\n",
    "            # This is the eccentricity prior from Kipping (2013) / https://arxiv.org/abs/1306.4982\n",
    "            BoundedBeta = pm.Bound(pm.Beta, lower=1e-5, upper=1-1e-5)\n",
    "            ecc = BoundedBeta(\"ecc\", alpha=0.867, beta=3.03, shape=n_pl,\n",
    "                              testval=np.tile(0.05,n_pl))\n",
    "            omega = xo.distributions.Angle(\"omega\", shape=n_pl, testval=np.tile(0.5,n_pl))\n",
    "\n",
    "        if use_GP:\n",
    "            self.gp = xo.gp.GP(kernel, self.lc['time'][self.lc['oot_mask']].astype(np.float32),\n",
    "                               self.lc['flux_err'][self.lc['oot_mask']]**2 + \\\n",
    "                               tt.dot(self.lc['flux_err_index'][self.lc['oot_mask']],tt.exp(logs2)),\n",
    "                               J=2)\n",
    "\n",
    "        ################################################\n",
    "        #     Creating function to generate transits\n",
    "        ################################################\n",
    "        def gen_lc(i_orbit,i_r,n_pl,mask=None,prefix=''):\n",
    "            # Short method to create stacked lightcurves, given some input time array and some input cadences:\n",
    "            # This function is needed because we may have \n",
    "            #   -  1) multiple cadences and \n",
    "            #   -  2) multiple telescopes (and therefore limb darkening coefficients)\n",
    "            trans_pred=[]\n",
    "            mask = ~np.isnan(self.lc['time']) if mask is None else mask\n",
    "            if np.sum(self.lc['tele_index'][:,0])>0:\n",
    "                trans_pred+=[xo.LimbDarkLightCurve(u_star_tess).get_light_curve(\n",
    "                                                         orbit=i_orbit, r=i_r,\n",
    "                                                         t=self.lc['time'][mask],\n",
    "                                                         texp=np.nanmedian(np.diff(self.lc['time'][mask]))\n",
    "                                                         )/(self.lc['flux_unit']*mult)]\n",
    "            else:\n",
    "                trans_pred+=[tt.zeros(( len(self.lc['time'][mask]),n_pl ))]\n",
    "\n",
    "            if np.sum(self.lc['tele_index'][:,1])>0:\n",
    "                trans_pred+=[xo.LimbDarkLightCurve(u_star_kep).get_light_curve(\n",
    "                                                         orbit=i_orbit, r=i_r,\n",
    "                                                         t=self.lc['time'][mask],\n",
    "                                                         texp=30/1440\n",
    "                                                         )/(self.lc['flux_unit']*mult)]\n",
    "            else:\n",
    "                trans_pred+=[tt.zeros(( len(self.lc['time'][mask]),n_pl ))]\n",
    "            # transit arrays (ntime x n_pls x 2) * telescope index (ntime x n_pls x 2), summed over dimension 2\n",
    "            return pm.Deterministic(prefix+\"light_curves\", \n",
    "                                    tt.sum(tt.stack(trans_pred,axis=-1) * \\\n",
    "                                           self.lc['tele_index'][self.lc['oot_mask']][:,np.newaxis,:],\n",
    "                                           axis=-1))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    def myloglikelihood(cube, ndim, nparams):\n",
    "        x_data, y_data, error_data = args\n",
    "        mod_data = some_model(cube)\n",
    "        return np.sum((-0.5 * ((y_data - mod_data) / error_data) ** 2))\n",
    "\n",
    "    pymultinest.run(myloglike, myprior, n_params)\n",
    "\n",
    "\n",
    "\n",
    "# model for 2 gaussians, same width, fixed offset\n",
    "def residuals(params):\n",
    "    #Adapted from the pymultinest \"model\" class - produces residual, thereby allowing us to use celerite GPs here\n",
    "    #input:\n",
    "    # - Dictionary of parameters with specific names\n",
    "    \n",
    "    #Computes transit model with exoplanet\n",
    "    \n",
    "    #\n",
    "    \n",
    "def prior(cube, ndim, nparams):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #cube[0] = cube[0]            # uniform prior between 0:1\n",
    "    cube[1] = 10**(cube[1]*8 - 4) # log-uniform prior between 10^-4 and 10^4\n",
    "    cube[2] = 10**(cube[2]*4 - 4) # log-uniform prior between 10^-4 and 1\n",
    "    if ndim < 4:\n",
    "        return\n",
    "    cube[3] = 10**(cube[3]*4 - 4) # log-uniform prior between 10^-4 and 1\n",
    "\n",
    "\n",
    "def loglike(cube, ndim, nparams):\n",
    "    pos1, width, height1 = cube[0], cube[1], cube[2]\n",
    "    height2 = cube[3] if ndim > 3 else 0\n",
    "    ymodel = model(pos1, width, height1, height2)\n",
    "    loglikelihood = (-0.5 * ((ymodel - ydata) / noise)**2).sum()\n",
    "    return loglikelihood\n",
    "\n",
    "parameters = [\"pos1\", \"width\", \"height1\"]\n",
    "n_params = len(parameters)\n",
    "\n",
    "pymultinest.run(loglike, prior, n_params, outputfiles_basename='test_1_', resume = False, verbose = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '51462' (I am process '50419')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /Users/hosborn/.theano/compiledir_Darwin-19.3.0-x86_64-i386-64bit-i386-3.7.6-64/lock_dir\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '51462' (I am process '50419')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /Users/hosborn/.theano/compiledir_Darwin-19.3.0-x86_64-i386-64bit-i386-3.7.6-64/lock_dir\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '51462' (I am process '50419')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /Users/hosborn/.theano/compiledir_Darwin-19.3.0-x86_64-i386-64bit-i386-3.7.6-64/lock_dir\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '52866' (I am process '50419')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /Users/hosborn/.theano/compiledir_Darwin-19.3.0-x86_64-i386-64bit-i386-3.7.6-64/lock_dir\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [a]\n",
      "Sampling 4 chains, 0 divergences: 100%|██████████| 4000/4000 [00:00<00:00, 5907.61draws/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "\n",
    "lb = np.array([0,1,0,2])\n",
    "ub = np.array([8,9,7,9])\n",
    "with pm.Model() as model:\n",
    "    BoundNormal = pm.Bound(pm.Normal,lower=lb, upper=ub)\n",
    "    a = BoundNormal(\"a\", mu=np.array([4,6,2,7]),sd=np.array([1,1,2,3]),shape=4)\n",
    "    trace = pm.sample()\n",
    "    \n",
    "assert np.all(np.logical_and(trace['a'] >lb, trace['a']<ub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
